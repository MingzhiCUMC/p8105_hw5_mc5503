---
title: "p8105_hw5_mc5503.Rmd"
author: "mc5503"
date: '2023-11-07'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(purrr)
```
# Problem 1

## data operations
```{r}
data1=read_csv("data/homicide-data.csv")|>janitor::clean_names()
str(data1)

time_range=range(data1|>pull(reported_date))
```
There are 52179 observations from `r as.Date(as.character(time_range[1]),format = "%Y%m%d")` to `r as.Date(as.character(time_range[2]),format = "%Y%m%d")`. There are `r ncol(data1)` variables, containing the time, location of each homicide and demographic characteristics of victim. There are total `r length(unique(data1|>pull(city)))` cities included in the data.

## calculate for Baltimore
```{r}
data1=data1|>mutate(city_state=paste(city,state,sep = ","))

n_obs=nrow(data1|>filter(city=='Baltimore'))

n_arrest=nrow(data1|>filter(city=='Baltimore'& disposition!="Closed by arrest"))

result=prop.test(x=n_arrest,n=n_obs)

p_est=broom::tidy(result)|>pull(estimate)

conf_low=broom::tidy(result)|>pull(conf.low)

conf_high=broom::tidy(result)|>pull(conf.high)

p_est

conf_low

conf_high
```

The proportion of unsolved (unarrested) homicide in Baltimore is `r p_est`, and the CI is [`r c(conf_low,conf_high)`].

## calculate for all cities
```{r}
prop_f=function(city_name)
{
  n_obs=nrow(data1|>filter(city==city_name))

  n_arrest=nrow(data1|>filter(city==city_name & disposition!="Closed by arrest"))

  result=prop.test(x=n_arrest,n=n_obs)

  p_est=broom::tidy(result)|>pull(estimate)

  conf_low=broom::tidy(result)|>pull(conf.low)

  conf_high=broom::tidy(result)|>pull(conf.high)
  
  tibble(p_est,conf_low,conf_high)
}

#prop_f("Baltimore")

city_list=unique(data1|>pull(city))

output = vector("list", length = length(city_list))

output=map(city_list,prop_f)

#data_nest=nest(data1, data = c(-city))

prop_results=tibble(city_list,output)|>unnest(cols = output)
```

## plots
```{r}
prop_results|>
  mutate(city_list=forcats::fct_reorder(city_list,p_est))|>
  ggplot(aes(x=city_list,y=p_est,ymin=conf_low,ymax=conf_high,color=city_list))+
  geom_point()+
  geom_errorbar()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5))+
  theme(legend.position="none")
```

# Problem 2
## tidy the dataset
```{r,warning=FALSE,message=FALSE}
file_names=list.files("data/data_long")

setwd("data/data_long")

data2=tibble(
  file_names,
  results=map(file_names,read_csv)
)|>unnest()|>
  separate(file_names,sep = "\\.",c("subject","csv"))|>
  pivot_longer(week_1:week_8,
               names_to = "time",
               values_to = "value")|>
  separate(time,c("unit","week"))|>
  select(subject,week,value)

(data2)
```
## spaghetti plot
```{r}
data2|>
  mutate(group=substr(subject,1,3))|>
  ggplot(aes(x=week,y=value,group=subject))+
  geom_line(aes(color=group))
```

In general, the control arm have lower values than the experimental arm. The the difference is the most significant in the 8th week.
The values of experimental group have an increasing trend over time.

# Problem 3
## mean=0
```{r}
n=30
sigma=5
mu=0
data_list=vector("list",length=5000)

for (i in 1:5000){
  data_list[[i]]=rnorm(n,mu,sigma)
}

get_results=function(data){
  broom::tidy(t.test(data))
}

data3=tibble(
  data=data_list,
  results=map(data_list,get_results)
)|>
  unnest(cols = results)|>
  select(data,estimate,p.value)

```

## repeat for other means
```{r}
df_results=function(mu,sigma=5,size=30)
{ 
  data_list=vector("list",length=5000)
  for (i in 1:5000)
  {
    data_list[[i]]=rnorm(n=size,mean =mu,sd=sigma)
  }
  
  tibble(
  data=data_list,
  results=map(data_list,get_results)
)|>
  unnest(cols = results)|>
  select(data,estimate,p.value)
}

data_all=tibble(
  mu=c(1,2,3,4,5,6),
  res=map(mu,df_results)
)


```

```{r}
prop_rej=function(df){
  nrow(df|>filter(p.value<0.05))/5000
}

data_all|>mutate(prop_rej_null=map(res,prop_rej))|>
  unnest(prop_rej_null)|>
  ggplot(aes(x=mu,y=prop_rej_null))+
  geom_point()
```

As the effect size increase, the power of the test increase and gradually approaches 1.

```{r}
get_avg_mu=function(df){
  mean(df|>pull(estimate))
}

p1=data_all|>mutate(avg_mu=map(res,get_avg_mu))|>
  unnest(avg_mu)|>
  ggplot(aes(x=mu,y=avg_mu))+
  geom_point(color="red",alpha=0.8)+
  scale_x_continuous(breaks = c(1,2,3,4,5,6))+
  scale_y_continuous(breaks = c(1,2,3,4,5,6))

get_avg_mu_rej=function(df){
  mean(df|>filter(p.value<0.05)|>pull(estimate))
}

data_p2=data_all|>mutate(avg_mu_rej=map(res,get_avg_mu_rej))|>
  unnest(avg_mu_rej)

p1+geom_point(data=data_p2,aes(x = mu, y = avg_mu_rej),color="yellow",alpha=0.8)
```

The sample average of μ_hat across tests for which the null is rejected is not equal to the true value of μ when true mean is small. This is because the sampling distribution of mean follows normal distribution with mean equals to true value of μ, but omitting the samples which the null isn't rejected will change the sampling distribution.

